{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "KSuliYqjdDy_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.11) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ypeleg/llama.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\"  \n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jjbHC7sYdFf-"
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"flickr8\", exist_ok=True)\n",
    "os.chdir(\"flickr8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qH4nc3CydGxW"
   },
   "outputs": [],
   "source": [
    "# !wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
    "# !wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n",
    "# !unzip -qq Flickr8k_Dataset.zip\n",
    "# !unzip -qq Flickr8k_text.zip\n",
    "# !rm Flickr8k_Dataset.zip Flickr8k_text.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nTiDd5KYdM1-",
    "outputId": "fa2295c4-8885-4309-929d-e2e996396804"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.1G\n",
      "-rw-r--r-- 1 qblocks qblocks 2.8M Oct 14  2013 CrowdFlowerAnnotations.txt\n",
      "-rw-r--r-- 1 qblocks qblocks 339K Oct 14  2013 ExpertAnnotations.txt\n",
      "drwxr-xr-x 1 qblocks qblocks 392K Oct  3  2012 Flicker8k_Dataset\n",
      "-rw-r--r-- 1 qblocks qblocks 3.1M Feb 16  2012 Flickr8k.lemma.token.txt\n",
      "-rw-r--r-- 1 qblocks qblocks 3.3M Oct 14  2013 Flickr8k.token.txt\n",
      "-rw-r--r-- 1 qblocks qblocks 1.1G Dec  6  2021 Flickr8k_Dataset.zip\n",
      "-rw-r--r-- 1 qblocks qblocks 2.3M Dec  6  2021 Flickr8k_text.zip\n",
      "-rw-r--r-- 1 qblocks qblocks  26K Oct 10  2013 Flickr_8k.devImages.txt\n",
      "-rw-r--r-- 1 qblocks qblocks  26K Oct 10  2013 Flickr_8k.testImages.txt\n",
      "-rw-r--r-- 1 qblocks qblocks 152K Oct 10  2013 Flickr_8k.trainImages.txt\n",
      "drwxrwxr-x 1 qblocks qblocks  124 Jul 15 14:49 __MACOSX\n",
      "drwxr-xr-x 1 qblocks qblocks   90 Jul 17 02:29 checkpoint\n",
      "drwxr-xr-x 1 qblocks qblocks  542 Jul 17 18:58 logs\n",
      "-rw-r--r-- 1 qblocks qblocks 1.8K Oct 14  2013 readme.txt\n",
      "drwxr-xr-x 1 qblocks qblocks    0 Jul 17 18:35 results\n"
     ]
    }
   ],
   "source": [
    "!ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NgC99X1jdUwZ",
    "outputId": "c430521a-e911-4008-88e6-4f81856730eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you use this corpus / data:\n",
      "\n",
      "Please cite: M. Hodosh, P. Young and J. Hockenmaier (2013) \"Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics\", Journal of Artifical Intellegence Research, Volume 47, pages 853-899\n",
      "http://www.jair.org/papers/paper3994.html\n",
      "\n",
      "\n",
      "Captions, Dataset Splits, and Human Annotations :\n",
      "\n",
      "\n",
      "Flickr8k.token.txt - the raw captions of the Flickr8k Dataset . The first column is the ID of the caption which is \"image address # caption number\"\n",
      "\n",
      "Flickr8k.lemma.txt - the lemmatized version of the above captions \n",
      "\n",
      "Flickr_8k.trainImages.txt - The training images used in our experiments\n",
      "Flickr_8k.devImages.txt - The development/validation images used in our experiments\n",
      "Flickr_8k.testImages.txt - The test images used in our experiments\n",
      "\n",
      "\n",
      "ExpertAnnotations.txt is the expert judgments.  The first two columns are the image and caption IDs.  Caption IDs are <image file name>#<0-4>.  The next three columns are the expert judgments for that image-caption pair.  Scores range from 1 to 4, with a 1 indicating that the caption does not describe the image at all, a 2 indicating the caption describes minor aspects of the image but does not describe the image, a 3 indicating that the caption almost describes the image with minor mistakes, and a 4 indicating that the caption describes the image.\n",
      "\n",
      "\n",
      "CrowdFlowerAnnotations.txt contains the CrowdFlower judgments.  The first two columns are the image and caption IDs.  The third column is the percent of Yeses, the fourth column is the total number of Yeses, the fifth column is the total number of Noes.  A Yes means that the caption describes the image (possibly with minor mistakes), while a No means that the caption does not describe the image.  Each image-caption pair has a minimum of three judgments, but some may have more.\n"
     ]
    }
   ],
   "source": [
    "!cat readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "ro29RZgLdV7X",
    "outputId": "d8141651-d6cd-440f-94ab-c7d25bf6caf7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "      <th>num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40455</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A man in a pink shirt climbs a rock face</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40456</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A man is rock climbing high in the air .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40457</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A person in a red shirt climbing up a rock fac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40458</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A rock climber in a red shirt .</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40459</th>\n",
       "      <td>997722733_0cb5439472.jpg</td>\n",
       "      <td>A rock climber practices on a rock climbing wa...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40460 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image  \\\n",
       "0      1000268201_693b08cb0e.jpg   \n",
       "1      1000268201_693b08cb0e.jpg   \n",
       "2      1000268201_693b08cb0e.jpg   \n",
       "3      1000268201_693b08cb0e.jpg   \n",
       "4      1000268201_693b08cb0e.jpg   \n",
       "...                          ...   \n",
       "40455   997722733_0cb5439472.jpg   \n",
       "40456   997722733_0cb5439472.jpg   \n",
       "40457   997722733_0cb5439472.jpg   \n",
       "40458   997722733_0cb5439472.jpg   \n",
       "40459   997722733_0cb5439472.jpg   \n",
       "\n",
       "                                                 caption num  \n",
       "0      A child in a pink dress is climbing up a set o...   0  \n",
       "1                  A girl going into a wooden building .   1  \n",
       "2       A little girl climbing into a wooden playhouse .   2  \n",
       "3      A little girl climbing the stairs to her playh...   3  \n",
       "4      A little girl in a pink dress going into a woo...   4  \n",
       "...                                                  ...  ..  \n",
       "40455           A man in a pink shirt climbs a rock face   0  \n",
       "40456           A man is rock climbing high in the air .   1  \n",
       "40457  A person in a red shirt climbing up a rock fac...   2  \n",
       "40458                    A rock climber in a red shirt .   3  \n",
       "40459  A rock climber practices on a rock climbing wa...   4  \n",
       "\n",
       "[40460 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Flickr8k.token.txt\", sep=\"\\t\", header=None)\n",
    "df.columns = [\"image\", \"caption\"]\n",
    "df['num'] = df.apply(lambda row: row['image'].split(\"#\")[-1], axis=1)\n",
    "df['image'] = df['image'].apply(lambda x: x.split(\"#\")[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nz8As08ddXgC",
    "outputId": "de2a41a0-3b4e-4a67-a818-be14b0728502"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 1000, 1000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images, validation_images, test_images = [], [], []\n",
    "\n",
    "with open(\"Flickr_8k.trainImages.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        train_images.append(line.strip())\n",
    "\n",
    "with open(\"Flickr_8k.devImages.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        validation_images.append(line.strip())\n",
    "\n",
    "with open(\"Flickr_8k.testImages.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        test_images.append(line.strip())\n",
    "\n",
    "len(train_images), len(validation_images), len(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images[:100]\n",
    "validation_images = validation_images[:20]\n",
    "test_images = test_images[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2idx = {img: idx + 1 for idx, img in enumerate(train_images + validation_images + test_images)}\n",
    "img2idx[\"<PAD>\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OL14pjdLdaWg",
    "outputId": "117f7388-709a-4189-fa80-0de84377b952"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000268201_693b08cb0e.jpg\n",
      "1001773457_577c3a7d70.jpg\n",
      "1002674143_1b742ab4b8.jpg\n",
      "1003163366_44323f5815.jpg\n",
      "1007129816_e794419615.jpg\n",
      "1007320043_627395c3d8.jpg\n",
      "1009434119_febe49276a.jpg\n",
      "1012212859_01547e3f17.jpg\n",
      "1015118661_980735411b.jpg\n",
      "1015584366_dfcec3c85a.jpg\n",
      "ls: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "!ls Flicker8k_Dataset | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8AIMr8JiFH-9"
   },
   "outputs": [],
   "source": [
    "def show_image(image, title=None):\n",
    "    image[0] = image[0] * 0.229\n",
    "    image[1] = image[1] * 0.224\n",
    "    image[2] = image[2] * 0.225\n",
    "    image[0] += 0.485\n",
    "    image[1] += 0.456\n",
    "    image[2] += 0.406\n",
    "\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "\n",
    "\n",
    "    plt.imshow(image)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3QJj47aFvTa"
   },
   "source": [
    "# **Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "aHS4VyuFFyy0"
   },
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        for param in resnet.parameters():\n",
    "            param.requires_grad_(False) # upto you if you want to make this True or False\n",
    "\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        features = self.resnet(images)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s1YrcJT2F_At",
    "outputId": "b9c1fd97-b381-4474-8acb-e00db2f92088"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qblocks/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/qblocks/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "encoder = EncoderCNN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-17 19:00:22.082715: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-17 19:00:23.605133: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/qblocks/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2023-07-17 19:00:23.605287: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/qblocks/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2023-07-17 19:00:23.605323: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/qblocks/nifty/flicker/llama\n",
      "/home/qblocks/nifty/flicker/flickr8\n"
     ]
    }
   ],
   "source": [
    "%cd ../llama/\n",
    "import llama\n",
    "%cd ../flickr8/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.04077768325805664,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 33,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae054ad1a3c456880cd6051feb5a2bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LLaMAForCausalLM(\n",
       "  (model): LLaMAModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=31999)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LLaMADecoderLayer(\n",
       "        (self_attn): LLaMAAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LLaMAMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL = 'decapoda-research/llama-7b-hf'\n",
    "\n",
    "tokenizer = llama.LLaMATokenizer.from_pretrained(MODEL)\n",
    "tokenizer.pad_token_id = 0\n",
    "\n",
    "original_model = llama.LLaMAForCausalLM.from_pretrained(MODEL)\n",
    "original_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLaMAForCausalLM(\n",
       "  (model): LLaMAModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=31999)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LLaMADecoderLayer(\n",
       "        (self_attn): LLaMAAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LLaMAMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [02:02<00:00,  1.94s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Flickr8kDataset(data.Dataset):\n",
    "    def __init__(self, image_dir, image_names, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_ids = image_names\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_id = self.image_ids[index]\n",
    "        image_file = f'{self.image_dir}/{image_id}'\n",
    "        image = Image.open(image_file).convert('RGB')\n",
    "        image = image.resize((512, 512))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, image_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    \n",
    "dataset = Flickr8kDataset(\"Flicker8k_Dataset\", list(img2idx.keys())[:-1], transform=transform)\n",
    "dataloader = data.DataLoader(dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "encoder.eval()\n",
    "img_emb = [torch.zeros((1, 2048))]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, name in tqdm(dataloader):\n",
    "        images = images.to(device)\n",
    "\n",
    "        out = encoder(images)\n",
    "        img_emb.append(out.cpu())\n",
    "\n",
    "img_emb = torch.cat(img_emb)\n",
    "\n",
    "\n",
    "# change Llama Embedding\n",
    "class CustomEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomEmbedding, self).__init__()\n",
    "        self.llama_emb = nn.Embedding(32000, 4096)\n",
    "        self.llama_emb.weight.data.copy_(original_model.model.embed_tokens.weight.data)\n",
    "\n",
    "        self.img_emb = nn.Embedding.from_pretrained(img_emb, freeze=True)\n",
    "        self.fc = nn.Linear(2048, 4096) # resnet.fc.in_features = 2048\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x - [batch, seq_len]\n",
    "        \"\"\"\n",
    "        llama = self.llama_emb(x[:, 1:])\n",
    "        img = self.fc(self.img_emb(x[:, :1]))\n",
    "        \n",
    "        return torch.cat([img, llama], dim=1)\n",
    "    \n",
    "original_model.model.embed_tokens = CustomEmbedding().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLaMAForCausalLM(\n",
       "  (model): LLaMAModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=31999)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LLaMADecoderLayer(\n",
       "        (self_attn): LLaMAAttention(\n",
       "          (q_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear8bitLt(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LLaMAMLP(\n",
       "          (gate_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear8bitLt(in_features=11008, out_features=4096, bias=False)\n",
       "          (up_proj): Linear8bitLt(in_features=4096, out_features=11008, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, caption = dataset[0]\n",
    "show_image(image, caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, caption = dataset[0]\n",
    "encoder(image[None, :].to(device)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.labels = []\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        \n",
    "        prompt = \"Write a caption for the image. \\nCaption: \\n\"\n",
    "        \n",
    "        for image_name, caption in data.values:\n",
    "            encodings_dict = tokenizer(prompt + caption, \n",
    "                                       truncation=True,\n",
    "                                       max_length=max_length, \n",
    "                                       padding=\"max_length\")\n",
    "            \n",
    "            input_ids = torch.tensor(encodings_dict['input_ids'])\n",
    "            input_ids[0] = img2idx[image_name]\n",
    "            \n",
    "            self.input_ids.append(input_ids)\n",
    "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
    "            \n",
    "    def __len__(self): \n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        return self.input_ids[idx], self.attn_masks[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max length\n",
    "max_length = max([len(tokenizer.encode(caption)) for image, caption, _ in df.values])\n",
    "\n",
    "# train and test dataset\n",
    "image_data = lambda images: pd.DataFrame(images, columns=[\"image\"]).merge(df.drop(\"num\", axis=1), how=\"inner\")\n",
    "\n",
    "train_data = TextDataset(image_data(train_images), \n",
    "                         tokenizer, \n",
    "                         max_length=max_length)\n",
    "\n",
    "\n",
    "test_data = TextDataset(image_data(test_images), \n",
    "                        tokenizer, \n",
    "                        max_length=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "                                  save_steps = 5000,\n",
    "                                  warmup_steps = 10,\n",
    "                                  logging_steps = 100,\n",
    "                                  weight_decay = 0.05,\n",
    "                                  num_train_epochs = 1,\n",
    "                                  logging_dir = './logs',\n",
    "                                  output_dir = './results',\n",
    "                                  per_device_eval_batch_size = 1,\n",
    "                                  per_device_train_batch_size = 1)\n",
    "\n",
    "Trainer(model=original_model,\n",
    "        args=training_args,\n",
    "        eval_dataset=val_dataset,\n",
    "        train_dataset=train_dataset,\n",
    "        data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]), 'attention_mask': torch.stack([f[1] for f in data]), 'labels': torch.stack([f[0] for f in data])}).train()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
